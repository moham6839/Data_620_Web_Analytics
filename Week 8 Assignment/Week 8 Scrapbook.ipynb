{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = [w.lower() for w in gen_all if w.isalpha() and w.lower() not in stopwords.words('english')] #remove special characters and numbers\n",
    "total_words = len(gen)  #get a raw count of significant words in the text\n",
    "gen_set = set(gen) #creates one copy of each unique word\n",
    "len(gen_set), total_words  #counts number of unique words vs total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = [w for w in text if w.isalpha()]\n",
    "# strip out stop words\n",
    "from nltk.corpus import stopwords\n",
    "emma = [w for w in emma if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e336c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist([word for word in gen]) #counts how many times each word occurs in text\n",
    "df = pd.DataFrame(fdist.most_common(len(gen_set)),columns=['word','counts'])  #create a dataframe of the top 2000 words\n",
    "\n",
    "words = 0\n",
    "distinct_count = 0\n",
    "for i in range(len(df)):\n",
    "    #print(df.iloc[i,0], df.iloc[i,1])\n",
    "    words += df.iloc[i, 1]\n",
    "    distinct_count = i\n",
    "    if words > total_words/2:\n",
    "        break\n",
    "\n",
    "print(i, words, total_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lem = lemmatizer.lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b02630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove single characters\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "# remove stopwords\n",
    "words = [word for word in words if word not in my_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9da56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard stopwords\n",
    "my_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "# additional stopwords\n",
    "my_stopwords = my_stopwords.union({\"'s\",\"'ll\",\"'re\",\"n't\",\"'ve\",\"'m\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lem = lemmatizer.lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e27b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many total unique words are in the corpus\n",
    "text_unique = set(text3)\n",
    "len(text_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.tokenize.word_tokenize(text)\n",
    "fdist1 = nltk.FreqDist(words)\n",
    "\n",
    "filtered_word_freq = dict((word, freq) for word, freq in fdist1.items() if not word.isdigit())\n",
    "\n",
    "print(filtered_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘', '.',\n",
    "            'said', 'and', 'but', 'the', '-', '–', '—', 'let', ',', 'get', '1', '1', 'like', 'lot', \"'s\",\"'ll\",\"'re\",\"n't\",\"'ve\",\"'m\"]\n",
    "sw_set = set(sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa21331",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99fb945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_text(TEXT):\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "    tokenized_speech = tokenizer.tokenize(TEXT)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    filtered_speech=[]\n",
    "    for w in tokenized_speech:\n",
    "        if w.lower() not in sw_set:\n",
    "            filtered_speech.append(w.lower())\n",
    "    \n",
    "    #tfidf = TfidfVectorizer(stop_words=sw_set)\n",
    "    \n",
    "    lemmatized_speech=[]\n",
    "    for w in filtered_speech:\n",
    "        lemmatized_speech.append(lemmatizer.lemmatize(w))\n",
    "        \n",
    "    return lemmatized_speech\n",
    "        \n",
    "#     fdist = FreqDist(lemmatized_speech)\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     fdist.plot(30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lemmatized(speech):\n",
    "    # Speeches is a df that contains a column Transcript which contains the speech\n",
    "    lem_df = pd.DataFrame(columns=['Lemmatized'])\n",
    "    lem_df['Lemmatized'] = speech['TEXT'].apply(lambda x: preprocessed_text(x))\n",
    "    tfidf = TfidfVectorizer(stop_words=sw_set)\n",
    "    \n",
    "    # This is going to hold all of the lemmatized words across all speeches\n",
    "    container = []\n",
    "    for i in range(len(lem_df)):\n",
    "        # Iterate through each lemmatized row and extend to container, want just 1 singular list\n",
    "        container.extend(lem_df['Lemmatized'].iloc[i])\n",
    "    fdist = FreqDist(container)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Word Count')\n",
    "    fdist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b19cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80b14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d3a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
